{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary tests\n",
    "TO test : OSVM, Isolation Forest, Decision tree, LocalOutlierFactor, NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  5803\n",
      "Number of variables :  36\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "from scipy.io import loadmat\n",
    "x = loadmat('data/satimage-2.mat')\n",
    "X = x[\"X\"]\n",
    "y = x[\"y\"]\n",
    "y = [val[0] for val in y]\n",
    "\n",
    "print(\"Number of observations : \",X.shape[0])\n",
    "print(\"Number of variables : \",X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion(y_pred, y=y):\n",
    "    labels = ['Inliers','Outlier']\n",
    "    sns.heatmap(confusion_matrix(y_true = y,y_pred = y_pred), xticklabels = labels, yticklabels = labels, annot = True, fmt='d', cmap=\"Reds\") \n",
    "    plt.title('Matrice de confusion')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# pyod library\n",
    "https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/\n",
    "\n",
    "TODO\n",
    "- nb of FP and FN as a function of contamination score for each method\n",
    "- compare with contamination score\n",
    "--> what happens if the contamination rate is not good at all ? under/overestimate ==> impact \n",
    "--> try with a wrong estimation (eg no shuffle?)\n",
    "--> theoretically \n",
    "- fine tuning :\n",
    "    - number of neighbors\n",
    "    - HBOS & AE ?\n",
    "- compare anomaly detection from outlier detection ? (use test & train?)\n",
    "- comparison of algos : how much is it sensitive to parameter tuning ? + add time estimate ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# # ! pip install pyod\n",
    "# knn\n",
    "from pyod.models.knn import KNN\n",
    "# angle based\n",
    "from pyod.models.abod import ABOD\n",
    "# histogram\n",
    "from pyod.models.hbos import HBOS\n",
    "# autoencoders\n",
    "from pyod.models.auto_encoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of outliers in the dataset :  0.012088477366255145\n"
     ]
    }
   ],
   "source": [
    "# estimate outlier fraction only based on trainset\n",
    "outlier_fraction = sum(y_train)/len(y_train)\n",
    "\n",
    "# # estimate outlier fraction only based on entire dataset\n",
    "# outlier_fraction = sum(y)/len(y)\n",
    "\n",
    "print(\"Fraction of outliers in the dataset : \", outlier_fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {}\n",
    "args[\"contamination\"] = [outlier_fraction, 1e-10, 1e-3, 1e-2, outlier_fraction/2, outlier_fraction*2, 0.5, 0.99]\n",
    "args[\"n_neighbors\"] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize classifier\n",
    "classifiers = {\n",
    "     'ABOD'   : ABOD(contamination=outlier_fraction, n_neighbors=args[\"n_neighbors\"]),\n",
    "     'KNN' :  KNN(contamination=outlier_fraction, n_neighbors=args[\"n_neighbors\"]),\n",
    "     'Average KNN': KNN(method='mean',contamination=outlier_fraction, n_neighbors=args[\"n_neighbors\"]),\n",
    "     'Median KNN': KNN(method='median',contamination=outlier_fraction, n_neighbors=args[\"n_neighbors\"]),\n",
    "     'HBOS' : HBOS(contamination=outlier_fraction),\n",
    "     'AutoEncoder' : AutoEncoder(contamination=outlier_fraction)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.012088477366255145, leaf_size=30,\n",
      "  method='largest', metric='minkowski', metric_params=None, n_jobs=1,\n",
      "  n_neighbors=5, p=2, radius=1.0)\n",
      "HBOS(alpha=0.1, contamination=0.012088477366255145, n_bins=10, tol=0.5)\n",
      "AutoEncoder(batch_size=32, contamination=0.012088477366255145,\n",
      "      dropout_rate=0.2, epochs=100, hidden_activation='relu',\n",
      "      hidden_neurons=[64, 32, 32, 64], l2_regularizer=0.1,\n",
      "      loss=<function mean_squared_error at 0x0000022CCDAB2678>,\n",
      "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
      "      random_state=None, validation_size=0.1, verbose=1)\n"
     ]
    }
   ],
   "source": [
    "print(classifiers[\"KNN\"])\n",
    "print(classifiers[\"HBOS\"])\n",
    "print(classifiers[\"AutoEncoder\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------ABOD---------------------\n",
      "Classifier with correct contamination :  ABOD(contamination=0.012088477366255145, method='fast', n_neighbors=5)\n",
      "\n",
      "----Contamination rate : 0.012088477366255145----\n",
      "threshold value :  4.609615751895344e-09\n",
      "Number of outliers :  25 \n",
      "Number of inliers :  1890\n",
      "Number of errors :  35\n",
      "Number of FP :  18\n",
      "Number of FN :  17\n",
      "\n",
      "----Contamination rate : 1e-10----\n",
      "threshold value :  7.093764999491746e-10\n",
      "Number of outliers :  0 \n",
      "Number of inliers :  1915\n",
      "Number of errors :  24\n",
      "Number of FP :  0\n",
      "Number of FN :  24\n",
      "\n",
      "----Contamination rate : 0.001----\n",
      "threshold value :  1.6135406340082427e-09\n",
      "Number of outliers :  1 \n",
      "Number of inliers :  1914\n",
      "Number of errors :  25\n",
      "Number of FP :  1\n",
      "Number of FN :  24\n",
      "\n",
      "----Contamination rate : 0.01----\n",
      "threshold value :  3.966925081167471e-09\n",
      "Number of outliers :  20 \n",
      "Number of inliers :  1895\n",
      "Number of errors :  30\n",
      "Number of FP :  13\n",
      "Number of FN :  17\n",
      "\n",
      "----Contamination rate : 0.006044238683127572----\n",
      "threshold value :  2.6264582291852604e-09\n",
      "Number of outliers :  18 \n",
      "Number of inliers :  1897\n",
      "Number of errors :  28\n",
      "Number of FP :  11\n",
      "Number of FN :  17\n",
      "\n",
      "----Contamination rate : 0.02417695473251029----\n",
      "threshold value :  6.8231995466333625e-09\n",
      "Number of outliers :  48 \n",
      "Number of inliers :  1867\n",
      "Number of errors :  52\n",
      "Number of FP :  38\n",
      "Number of FN :  14\n",
      "\n",
      "----Contamination rate : 0.5----\n",
      "threshold value :  9.577672904561261e-08\n",
      "Number of outliers :  997 \n",
      "Number of inliers :  918\n",
      "Number of errors :  977\n",
      "Number of FP :  975\n",
      "Number of FN :  2\n",
      "\n",
      "----Contamination rate : 0.99----\n",
      "threshold value :  6.748673302253912e-07\n",
      "Number of outliers :  1892 \n",
      "Number of inliers :  23\n",
      "Number of errors :  1868\n",
      "Number of FP :  1868\n",
      "Number of FN :  0\n",
      "\n",
      "---------------------KNN---------------------\n",
      "Classifier with correct contamination :  KNN(algorithm='auto', contamination=0.012088477366255145, leaf_size=30,\n",
      "  method='largest', metric='minkowski', metric_params=None, n_jobs=1,\n",
      "  n_neighbors=5, p=2, radius=1.0)\n",
      "\n",
      "----Contamination rate : 0.012088477366255145----\n",
      "threshold value :  -60.501434098368335\n",
      "Number of outliers :  31 \n",
      "Number of inliers :  1884\n",
      "Number of errors :  31\n",
      "Number of FP :  19\n",
      "Number of FN :  12\n",
      "\n",
      "----Contamination rate : 1e-10----\n",
      "threshold value :  -114.48580546896169\n",
      "Number of outliers :  1 \n",
      "Number of inliers :  1914\n",
      "Number of errors :  23\n",
      "Number of FP :  0\n",
      "Number of FN :  23\n",
      "\n",
      "----Contamination rate : 0.001----\n",
      "threshold value :  -102.15231136507853\n",
      "Number of outliers :  4 \n",
      "Number of inliers :  1911\n",
      "Number of errors :  20\n",
      "Number of FP :  0\n",
      "Number of FN :  20\n",
      "\n",
      "----Contamination rate : 0.01----\n",
      "threshold value :  -66.35076950346502\n",
      "Number of outliers :  23 \n",
      "Number of inliers :  1892\n",
      "Number of errors :  25\n",
      "Number of FP :  12\n",
      "Number of FN :  13\n",
      "\n",
      "----Contamination rate : 0.006044238683127572----\n",
      "threshold value :  -76.17667333491033\n",
      "Number of outliers :  15 \n",
      "Number of inliers :  1900\n",
      "Number of errors :  21\n",
      "Number of FP :  6\n",
      "Number of FN :  15\n",
      "\n",
      "----Contamination rate : 0.02417695473251029----\n",
      "threshold value :  -53.08390482681756\n",
      "Number of outliers :  60 \n",
      "Number of inliers :  1855\n",
      "Number of errors :  54\n",
      "Number of FP :  45\n",
      "Number of FN :  9\n",
      "\n",
      "----Contamination rate : 0.5----\n",
      "threshold value :  -23.643180835073778\n",
      "Number of outliers :  945 \n",
      "Number of inliers :  970\n",
      "Number of errors :  921\n",
      "Number of FP :  921\n",
      "Number of FN :  0\n",
      "\n",
      "----Contamination rate : 0.99----\n",
      "threshold value :  -15.500951597008086\n",
      "Number of outliers :  1897 \n",
      "Number of inliers :  18\n",
      "Number of errors :  1873\n",
      "Number of FP :  1873\n",
      "Number of FN :  0\n",
      "\n",
      "---------------------Average KNN---------------------\n",
      "Classifier with correct contamination :  KNN(algorithm='auto', contamination=0.012088477366255145, leaf_size=30,\n",
      "  method='mean', metric='minkowski', metric_params=None, n_jobs=1,\n",
      "  n_neighbors=5, p=2, radius=1.0)\n",
      "\n",
      "----Contamination rate : 0.012088477366255145----\n",
      "threshold value :  -53.95650660324902\n",
      "Number of outliers :  29 \n",
      "Number of inliers :  1886\n",
      "Number of errors :  29\n",
      "Number of FP :  17\n",
      "Number of FN :  12\n",
      "\n",
      "----Contamination rate : 1e-10----\n",
      "threshold value :  -91.16315161936468\n",
      "Number of outliers :  0 \n",
      "Number of inliers :  1915\n",
      "Number of errors :  24\n",
      "Number of FP :  0\n",
      "Number of FN :  24\n",
      "\n",
      "----Contamination rate : 0.001----\n",
      "threshold value :  -82.63258618690483\n",
      "Number of outliers :  1 \n",
      "Number of inliers :  1914\n",
      "Number of errors :  23\n",
      "Number of FP :  0\n",
      "Number of FN :  23\n",
      "\n",
      "----Contamination rate : 0.01----\n",
      "threshold value :  -55.75669701016491\n",
      "Number of outliers :  24 \n",
      "Number of inliers :  1891\n",
      "Number of errors :  26\n",
      "Number of FP :  13\n",
      "Number of FN :  13\n",
      "\n",
      "----Contamination rate : 0.006044238683127572----\n",
      "threshold value :  -65.99458395922902\n",
      "Number of outliers :  16 \n",
      "Number of inliers :  1899\n",
      "Number of errors :  22\n",
      "Number of FP :  7\n",
      "Number of FN :  15\n",
      "\n",
      "----Contamination rate : 0.02417695473251029----\n",
      "threshold value :  -48.102841417412805\n",
      "Number of outliers :  61 \n",
      "Number of inliers :  1854\n",
      "Number of errors :  57\n",
      "Number of FP :  47\n",
      "Number of FN :  10\n",
      "\n",
      "----Contamination rate : 0.5----\n",
      "threshold value :  -22.219478054150173\n",
      "Number of outliers :  958 \n",
      "Number of inliers :  957\n",
      "Number of errors :  934\n",
      "Number of FP :  934\n",
      "Number of FN :  0\n",
      "\n",
      "----Contamination rate : 0.99----\n",
      "threshold value :  -14.74811572295891\n",
      "Number of outliers :  1900 \n",
      "Number of inliers :  15\n",
      "Number of errors :  1876\n",
      "Number of FP :  1876\n",
      "Number of FN :  0\n",
      "\n",
      "---------------------Median KNN---------------------\n",
      "Classifier with correct contamination :  KNN(algorithm='auto', contamination=0.012088477366255145, leaf_size=30,\n",
      "  method='median', metric='minkowski', metric_params=None, n_jobs=1,\n",
      "  n_neighbors=5, p=2, radius=1.0)\n",
      "\n",
      "----Contamination rate : 0.012088477366255145----\n",
      "threshold value :  -56.16514842944255\n",
      "Number of outliers :  28 \n",
      "Number of inliers :  1887\n",
      "Number of errors :  28\n",
      "Number of FP :  16\n",
      "Number of FN :  12\n",
      "\n",
      "----Contamination rate : 1e-10----\n",
      "threshold value :  -96.79875922432207\n",
      "Number of outliers :  0 \n",
      "Number of inliers :  1915\n",
      "Number of errors :  24\n",
      "Number of FP :  0\n",
      "Number of FN :  24\n",
      "\n",
      "----Contamination rate : 0.001----\n",
      "threshold value :  -90.31431570785358\n",
      "Number of outliers :  1 \n",
      "Number of inliers :  1914\n",
      "Number of errors :  23\n",
      "Number of FP :  0\n",
      "Number of FN :  23\n",
      "\n",
      "----Contamination rate : 0.01----\n",
      "threshold value :  -58.51661897209347\n",
      "Number of outliers :  25 \n",
      "Number of inliers :  1890\n",
      "Number of errors :  25\n",
      "Number of FP :  13\n",
      "Number of FN :  12\n",
      "\n",
      "----Contamination rate : 0.006044238683127572----\n",
      "threshold value :  -69.08491226803667\n",
      "Number of outliers :  16 \n",
      "Number of inliers :  1899\n",
      "Number of errors :  22\n",
      "Number of FP :  7\n",
      "Number of FN :  15\n",
      "\n",
      "----Contamination rate : 0.02417695473251029----\n",
      "threshold value :  -49.47904347061654\n",
      "Number of outliers :  64 \n",
      "Number of inliers :  1851\n",
      "Number of errors :  58\n",
      "Number of FP :  49\n",
      "Number of FN :  9\n",
      "\n",
      "----Contamination rate : 0.5----\n",
      "threshold value :  -22.516660498395403\n",
      "Number of outliers :  957 \n",
      "Number of inliers :  958\n",
      "Number of errors :  933\n",
      "Number of FP :  933\n",
      "Number of FN :  0\n",
      "\n",
      "----Contamination rate : 0.99----\n",
      "threshold value :  -14.711177874934034\n",
      "Number of outliers :  1895 \n",
      "Number of inliers :  20\n",
      "Number of errors :  1871\n",
      "Number of FP :  1871\n",
      "Number of FN :  0\n",
      "\n",
      "---------------------HBOS---------------------\n",
      "Classifier with correct contamination :  HBOS(alpha=0.1, contamination=0.012088477366255145, n_bins=10, tol=0.5)\n",
      "\n",
      "----Contamination rate : 0.012088477366255145----\n",
      "threshold value :  -115.74851423524497\n",
      "Number of outliers :  21 \n",
      "Number of inliers :  1894\n",
      "Number of errors :  15\n",
      "Number of FP :  6\n",
      "Number of FN :  9\n",
      "\n",
      "----Contamination rate : 1e-10----\n",
      "threshold value :  -118.55195228296937\n",
      "Number of outliers :  0 \n",
      "Number of inliers :  1915\n",
      "Number of errors :  24\n",
      "Number of FP :  0\n",
      "Number of FN :  24\n",
      "\n",
      "----Contamination rate : 0.001----\n",
      "threshold value :  -118.1930342874137\n",
      "Number of outliers :  2 \n",
      "Number of inliers :  1913\n",
      "Number of errors :  22\n",
      "Number of FP :  0\n",
      "Number of FN :  22\n",
      "\n",
      "----Contamination rate : 0.01----\n",
      "threshold value :  -115.8917669055348\n",
      "Number of outliers :  16 \n",
      "Number of inliers :  1899\n",
      "Number of errors :  16\n",
      "Number of FP :  4\n",
      "Number of FN :  12\n",
      "\n",
      "----Contamination rate : 0.006044238683127572----\n",
      "threshold value :  -116.45338101530739\n",
      "Number of outliers :  9 \n",
      "Number of inliers :  1906\n",
      "Number of errors :  15\n",
      "Number of FP :  0\n",
      "Number of FN :  15\n",
      "\n",
      "----Contamination rate : 0.02417695473251029----\n",
      "threshold value :  -114.98962551766098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of outliers :  48 \n",
      "Number of inliers :  1867\n",
      "Number of errors :  36\n",
      "Number of FP :  30\n",
      "Number of FN :  6\n",
      "\n",
      "----Contamination rate : 0.5----\n",
      "threshold value :  -110.34817464326946\n",
      "Number of outliers :  969 \n",
      "Number of inliers :  946\n",
      "Number of errors :  945\n",
      "Number of FP :  945\n",
      "Number of FN :  0\n",
      "\n",
      "----Contamination rate : 0.99----\n",
      "threshold value :  -108.79668130726404\n",
      "Number of outliers :  1902 \n",
      "Number of inliers :  13\n",
      "Number of errors :  1878\n",
      "Number of FP :  1878\n",
      "Number of FN :  0\n",
      "\n",
      "---------------------AutoEncoder---------------------\n",
      "Classifier with correct contamination :  AutoEncoder(batch_size=32, contamination=0.012088477366255145,\n",
      "      dropout_rate=0.2, epochs=100, hidden_activation='relu',\n",
      "      hidden_neurons=[64, 32, 32, 64], l2_regularizer=0.1,\n",
      "      loss=<function mean_squared_error at 0x0000022CCDAB2678>,\n",
      "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
      "      random_state=None, validation_size=0.1, verbose=1)\n",
      "\n",
      "----Contamination rate : 0.012088477366255145----\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 3s 994us/step - loss: 59.2066 - val_loss: 24.4600\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 15.3261 - val_loss: 11.5661\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 411us/step - loss: 9.2665 - val_loss: 7.9692\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 294us/step - loss: 6.9182 - val_loss: 6.2606\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 5.6152 - val_loss: 5.2451\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 228us/step - loss: 4.8006 - val_loss: 4.5634\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 4.2252 - val_loss: 4.0759\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 3.7969 - val_loss: 3.7063\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 3.4661 - val_loss: 3.4153\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 3.2064 - val_loss: 3.1875\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 3.0034 - val_loss: 2.9964\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 288us/step - loss: 2.8229 - val_loss: 2.8390\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 302us/step - loss: 2.6813 - val_loss: 2.7046\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 239us/step - loss: 2.5587 - val_loss: 2.5904\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 343us/step - loss: 2.4483 - val_loss: 2.4891\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 316us/step - loss: 2.3518 - val_loss: 2.4017\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 2.2692 - val_loss: 2.3232\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 2.1976 - val_loss: 2.2535\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - ETA: 0s - loss: 2.127 - 1s 270us/step - loss: 2.1252 - val_loss: 2.1887\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 241us/step - loss: 2.0633 - val_loss: 2.1312\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 253us/step - loss: 2.0119 - val_loss: 2.0802\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 320us/step - loss: 1.9678 - val_loss: 2.0320\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 414us/step - loss: 1.9106 - val_loss: 1.9869\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.8677 - val_loss: 1.9460\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 287us/step - loss: 1.8312 - val_loss: 1.9075\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 353us/step - loss: 1.7940 - val_loss: 1.8724\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 2s 498us/step - loss: 1.7587 - val_loss: 1.8386\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 336us/step - loss: 1.7249 - val_loss: 1.8080\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 2s 498us/step - loss: 1.6929 - val_loss: 1.7791\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 267us/step - loss: 1.6652 - val_loss: 1.7514\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 272us/step - loss: 1.6361 - val_loss: 1.7258\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.6074 - val_loss: 1.7009\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 230us/step - loss: 1.5840 - val_loss: 1.6770\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 282us/step - loss: 1.5642 - val_loss: 1.6554\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 294us/step - loss: 1.5345 - val_loss: 1.6352\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 288us/step - loss: 1.5174 - val_loss: 1.6137\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 330us/step - loss: 1.4965 - val_loss: 1.5961\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 312us/step - loss: 1.4800 - val_loss: 1.5776\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 294us/step - loss: 1.4592 - val_loss: 1.5594\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 261us/step - loss: 1.4407 - val_loss: 1.5428\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 259us/step - loss: 1.4231 - val_loss: 1.5261\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 267us/step - loss: 1.4075 - val_loss: 1.5109\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 290us/step - loss: 1.3909 - val_loss: 1.4959\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 290us/step - loss: 1.3753 - val_loss: 1.4821\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 265us/step - loss: 1.3629 - val_loss: 1.4692\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 250us/step - loss: 1.3470 - val_loss: 1.4551\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 262us/step - loss: 1.3338 - val_loss: 1.4426\n",
      "Epoch 48/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 305us/step - loss: 1.3232 - val_loss: 1.4307\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 248us/step - loss: 1.3091 - val_loss: 1.4187\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 279us/step - loss: 1.2985 - val_loss: 1.4075\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 393us/step - loss: 1.2853 - val_loss: 1.3971\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 284us/step - loss: 1.2721 - val_loss: 1.3868\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 1.2624 - val_loss: 1.3761\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.2531 - val_loss: 1.3669\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 239us/step - loss: 1.2413 - val_loss: 1.3576\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.2333 - val_loss: 1.3480\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.2224 - val_loss: 1.3389\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 1.2146 - val_loss: 1.3307\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 260us/step - loss: 1.2063 - val_loss: 1.3225\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 295us/step - loss: 1.1970 - val_loss: 1.3149\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.1890 - val_loss: 1.3076\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.1794 - val_loss: 1.2991\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.1736 - val_loss: 1.2924\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 232us/step - loss: 1.1648 - val_loss: 1.2850\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.1567 - val_loss: 1.2783\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 232us/step - loss: 1.1511 - val_loss: 1.2722\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 225us/step - loss: 1.1446 - val_loss: 1.2658\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 243us/step - loss: 1.1383 - val_loss: 1.2596\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 240us/step - loss: 1.1317 - val_loss: 1.2537\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.1260 - val_loss: 1.2482\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 235us/step - loss: 1.1194 - val_loss: 1.2428\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 231us/step - loss: 1.1132 - val_loss: 1.2374\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.1085 - val_loss: 1.2326\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 240us/step - loss: 1.1031 - val_loss: 1.2274\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.0978 - val_loss: 1.2230\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.0932 - val_loss: 1.2182\n",
      "Epoch 77/100\n",
      "3499/3499 [==============================] - 1s 244us/step - loss: 1.0885 - val_loss: 1.2137\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.0835 - val_loss: 1.2096\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.0795 - val_loss: 1.2056\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 1.0751 - val_loss: 1.2016\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.0711 - val_loss: 1.1977\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 261us/step - loss: 1.0671 - val_loss: 1.1942\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 1.0634 - val_loss: 1.1906\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.0595 - val_loss: 1.1872\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.0565 - val_loss: 1.1841\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.0529 - val_loss: 1.1807\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.0497 - val_loss: 1.1778\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 261us/step - loss: 1.0466 - val_loss: 1.1749\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.0435 - val_loss: 1.1722\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.0406 - val_loss: 1.1696\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.0378 - val_loss: 1.1670\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.0354 - val_loss: 1.1646\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0327 - val_loss: 1.1623\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.0303 - val_loss: 1.1600\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 239us/step - loss: 1.0280 - val_loss: 1.1578\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.0258 - val_loss: 1.1558\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 1.0237 - val_loss: 1.1538\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.0219 - val_loss: 1.1519\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 226us/step - loss: 1.0197 - val_loss: 1.1501\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 231us/step - loss: 1.0181 - val_loss: 1.1485\n",
      "threshold value :  -11.508417724024943\n",
      "Number of outliers :  26 \n",
      "Number of inliers :  1889\n",
      "Number of errors :  12\n",
      "Number of FP :  7\n",
      "Number of FN :  5\n",
      "\n",
      "----Contamination rate : 1e-10----\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 2s 676us/step - loss: 61.8257 - val_loss: 21.0893\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 233us/step - loss: 15.7237 - val_loss: 10.9917\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 200us/step - loss: 9.5886 - val_loss: 7.7030\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 192us/step - loss: 7.1070 - val_loss: 6.0537\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 192us/step - loss: 5.7255 - val_loss: 5.0450\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 4.8822 - val_loss: 4.3810\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 191us/step - loss: 4.2926 - val_loss: 3.9082\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 3.8825 - val_loss: 3.5497\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 3.5515 - val_loss: 3.2720\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 3.2844 - val_loss: 3.0483\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 3.0782 - val_loss: 2.8657\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 2.9013 - val_loss: 2.7119\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 2.7509 - val_loss: 2.5806\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 2.6224 - val_loss: 2.4671\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 2.5135 - val_loss: 2.3677\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 2.4188 - val_loss: 2.2800\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 2.3359 - val_loss: 2.2024\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 2.2573 - val_loss: 2.1332\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 2.1844 - val_loss: 2.0684\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 2.1280 - val_loss: 2.0102\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 2.0684 - val_loss: 1.9581\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 2.0123 - val_loss: 1.9080\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 1.9700 - val_loss: 1.8644\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.9192 - val_loss: 1.8214\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.8768 - val_loss: 1.7839\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.8387 - val_loss: 1.7459\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.8053 - val_loss: 1.7113\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.7679 - val_loss: 1.6805\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.7380 - val_loss: 1.6498\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 260us/step - loss: 1.7065 - val_loss: 1.6211\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.6784 - val_loss: 1.5937\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.6452 - val_loss: 1.5679\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 1.6224 - val_loss: 1.5453\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.5954 - val_loss: 1.5210\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.5765 - val_loss: 1.4996\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.5544 - val_loss: 1.4784\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.5317 - val_loss: 1.4578\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 242us/step - loss: 1.5088 - val_loss: 1.4393\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 233us/step - loss: 1.4906 - val_loss: 1.4221\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.4706 - val_loss: 1.4037\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.4543 - val_loss: 1.3874\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 1.4369 - val_loss: 1.3700\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4197 - val_loss: 1.3542\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.4044 - val_loss: 1.3397\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.3919 - val_loss: 1.3260\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.3734 - val_loss: 1.3118\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.3597 - val_loss: 1.2987\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.3459 - val_loss: 1.2867\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.3313 - val_loss: 1.2732\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.3206 - val_loss: 1.2614\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.3079 - val_loss: 1.2504\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2957 - val_loss: 1.2399\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.2844 - val_loss: 1.2284\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2741 - val_loss: 1.2181\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.2635 - val_loss: 1.2085\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.2524 - val_loss: 1.1985\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.2427 - val_loss: 1.1894\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.2337 - val_loss: 1.1806\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.2233 - val_loss: 1.1720\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.2150 - val_loss: 1.1640\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.2072 - val_loss: 1.1562\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 223us/step - loss: 1.1986 - val_loss: 1.1482\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.1899 - val_loss: 1.1408\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.1822 - val_loss: 1.1335\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.1746 - val_loss: 1.1266\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.1686 - val_loss: 1.1202\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.1612 - val_loss: 1.1133\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1550 - val_loss: 1.1074\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.1480 - val_loss: 1.1013\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.1410 - val_loss: 1.0955\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.1361 - val_loss: 1.0897\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.1303 - val_loss: 1.0842\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.1238 - val_loss: 1.0792\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1194 - val_loss: 1.0740\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.1135 - val_loss: 1.0692\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.1088 - val_loss: 1.0645\n",
      "Epoch 77/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.1037 - val_loss: 1.0601\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.0993 - val_loss: 1.0560\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0943 - val_loss: 1.0518\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0908 - val_loss: 1.0476\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0857 - val_loss: 1.0438\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0821 - val_loss: 1.0402\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0787 - val_loss: 1.0367\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0752 - val_loss: 1.0332\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 1.0714 - val_loss: 1.0299\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 188us/step - loss: 1.0682 - val_loss: 1.0268\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.0650 - val_loss: 1.0244\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0618 - val_loss: 1.0211\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 191us/step - loss: 1.0587 - val_loss: 1.0182\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0559 - val_loss: 1.0155\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0531 - val_loss: 1.0129\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0507 - val_loss: 1.0106\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0480 - val_loss: 1.0082\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0457 - val_loss: 1.0060\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0433 - val_loss: 1.0039\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0412 - val_loss: 1.0019\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.0390 - val_loss: 1.0000\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0371 - val_loss: 0.9981\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0352 - val_loss: 0.9965\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0336 - val_loss: 0.9947\n",
      "threshold value :  -18.009697666926254\n",
      "Number of outliers :  0 \n",
      "Number of inliers :  1915\n",
      "Number of errors :  24\n",
      "Number of FP :  0\n",
      "Number of FN :  24\n",
      "\n",
      "----Contamination rate : 0.001----\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 2s 622us/step - loss: 57.5504 - val_loss: 21.2143\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 15.0203 - val_loss: 10.5023\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 8.9922 - val_loss: 7.4483\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 6.7876 - val_loss: 5.9412\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 5.5623 - val_loss: 5.0319\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 4.7948 - val_loss: 4.4134\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 4.2585 - val_loss: 3.9622\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 3.8617 - val_loss: 3.6100\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 3.5388 - val_loss: 3.3336\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 3.2820 - val_loss: 3.1109\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 3.0858 - val_loss: 2.9238\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 2.9050 - val_loss: 2.7681\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 2.7559 - val_loss: 2.6324\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 2.6424 - val_loss: 2.5165\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 2.5220 - val_loss: 2.4142\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 2.4336 - val_loss: 2.3235\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 2.3340 - val_loss: 2.2409\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 2.2574 - val_loss: 2.1691\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 2.1947 - val_loss: 2.1033\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 2.1292 - val_loss: 2.0421\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 2.0721 - val_loss: 1.9870\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 2.0128 - val_loss: 1.9366\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.9654 - val_loss: 1.8897\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.9235 - val_loss: 1.8468\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.8728 - val_loss: 1.8061\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.8399 - val_loss: 1.7684\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.8015 - val_loss: 1.7333\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.7646 - val_loss: 1.7000\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.7372 - val_loss: 1.6697\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.7030 - val_loss: 1.6398\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.6773 - val_loss: 1.6123\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.6473 - val_loss: 1.5851\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.6217 - val_loss: 1.5609\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.5979 - val_loss: 1.5375\n",
      "Epoch 35/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.5717 - val_loss: 1.5145\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.5501 - val_loss: 1.4929\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.5283 - val_loss: 1.4729\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.5055 - val_loss: 1.4534\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.4896 - val_loss: 1.4352\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.4685 - val_loss: 1.4169\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.4525 - val_loss: 1.4004\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.4364 - val_loss: 1.3845\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.4173 - val_loss: 1.3686\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.4015 - val_loss: 1.3533\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.3864 - val_loss: 1.3389\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.3727 - val_loss: 1.3253\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 192us/step - loss: 1.3592 - val_loss: 1.3120\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.3457 - val_loss: 1.2997\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.3323 - val_loss: 1.2867\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.3209 - val_loss: 1.2748\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.3069 - val_loss: 1.2637\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.2957 - val_loss: 1.2523\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2862 - val_loss: 1.2415\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.2755 - val_loss: 1.2321\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2633 - val_loss: 1.2215\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 228us/step - loss: 1.2526 - val_loss: 1.2123\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 1.2446 - val_loss: 1.2031\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.2342 - val_loss: 1.1940\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.2239 - val_loss: 1.1853\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.2175 - val_loss: 1.1770\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.2081 - val_loss: 1.1693\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.2003 - val_loss: 1.1617\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.1914 - val_loss: 1.1537\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.1844 - val_loss: 1.1465\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.1773 - val_loss: 1.1396\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1701 - val_loss: 1.1333\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.1627 - val_loss: 1.1266\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.1558 - val_loss: 1.1201\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.1497 - val_loss: 1.1140\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.1437 - val_loss: 1.1083\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.1368 - val_loss: 1.1027\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.1312 - val_loss: 1.0974\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.1261 - val_loss: 1.0921\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1202 - val_loss: 1.0869\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.1159 - val_loss: 1.0820\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.1104 - val_loss: 1.0774\n",
      "Epoch 77/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.1053 - val_loss: 1.0728\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.1011 - val_loss: 1.0686\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0964 - val_loss: 1.0643\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.0921 - val_loss: 1.0601\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0873 - val_loss: 1.0563\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0840 - val_loss: 1.0525\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0802 - val_loss: 1.0490\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.0761 - val_loss: 1.0455\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 208us/step - loss: 1.0726 - val_loss: 1.0421\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0693 - val_loss: 1.0390\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.0664 - val_loss: 1.0359\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.0626 - val_loss: 1.0329\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 233us/step - loss: 1.0598 - val_loss: 1.0300\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0569 - val_loss: 1.0273\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.0545 - val_loss: 1.0247\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.0515 - val_loss: 1.0223\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.0493 - val_loss: 1.0199\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.0465 - val_loss: 1.0175\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0443 - val_loss: 1.0153\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0421 - val_loss: 1.0132\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0400 - val_loss: 1.0112\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0378 - val_loss: 1.0093\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0359 - val_loss: 1.0075\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0342 - val_loss: 1.0057\n",
      "threshold value :  -17.26534800193918\n",
      "Number of outliers :  2 \n",
      "Number of inliers :  1913\n",
      "Number of errors :  22\n",
      "Number of FP :  0\n",
      "Number of FN :  22\n",
      "\n",
      "----Contamination rate : 0.01----\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_22 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 2s 653us/step - loss: 54.6553 - val_loss: 20.2179\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 14.1300 - val_loss: 11.0568\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 8.9461 - val_loss: 7.8962\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 6.7171 - val_loss: 6.2346\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 5.4638 - val_loss: 5.2279\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 4.6949 - val_loss: 4.5407\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 4.1503 - val_loss: 4.0460\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 3.7387 - val_loss: 3.6721\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 3.4196 - val_loss: 3.3793\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 3.1702 - val_loss: 3.1462\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 2.9775 - val_loss: 2.9563\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 2.8016 - val_loss: 2.7975\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 2.6669 - val_loss: 2.6626\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 2.5367 - val_loss: 2.5497\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 2.4382 - val_loss: 2.4469\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 2.3486 - val_loss: 2.3590\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 2.2631 - val_loss: 2.2806\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 2.1937 - val_loss: 2.2109\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 2.1308 - val_loss: 2.1466\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 2.0685 - val_loss: 2.0894\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 2.0122 - val_loss: 2.0370\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 223us/step - loss: 1.9620 - val_loss: 1.9901\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.9178 - val_loss: 1.9448\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.8807 - val_loss: 1.9055\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.8394 - val_loss: 1.8656\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.8009 - val_loss: 1.8302\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.7641 - val_loss: 1.7959\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.7297 - val_loss: 1.7641\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 228us/step - loss: 1.7024 - val_loss: 1.7340\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.6722 - val_loss: 1.7049\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.6451 - val_loss: 1.6780\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.6172 - val_loss: 1.6530\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.5934 - val_loss: 1.6299\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.5733 - val_loss: 1.6094\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.5496 - val_loss: 1.5861\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.5297 - val_loss: 1.5648\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.5030 - val_loss: 1.5446\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4849 - val_loss: 1.5256\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.4686 - val_loss: 1.5091\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.4484 - val_loss: 1.4908\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4310 - val_loss: 1.4735\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.4152 - val_loss: 1.4586\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.4011 - val_loss: 1.4449\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.3873 - val_loss: 1.4297\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.3704 - val_loss: 1.4144\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.3563 - val_loss: 1.4010\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.3412 - val_loss: 1.3884\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.3289 - val_loss: 1.3763\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.3167 - val_loss: 1.3637\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.3042 - val_loss: 1.3518\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.2923 - val_loss: 1.3403\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.2802 - val_loss: 1.3304\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 1.2689 - val_loss: 1.3192\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.2567 - val_loss: 1.3094\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.2489 - val_loss: 1.2997\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.2395 - val_loss: 1.2901\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.2299 - val_loss: 1.2815\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.2182 - val_loss: 1.2725\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.2101 - val_loss: 1.2651\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.2009 - val_loss: 1.2562\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.1920 - val_loss: 1.2495\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1856 - val_loss: 1.2415\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1775 - val_loss: 1.2338\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.1703 - val_loss: 1.2263\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.1633 - val_loss: 1.2200\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.1562 - val_loss: 1.2139\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.1483 - val_loss: 1.2070\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 225us/step - loss: 1.1421 - val_loss: 1.2010\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.1359 - val_loss: 1.1950\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.1300 - val_loss: 1.1892\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.1242 - val_loss: 1.1836\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.1186 - val_loss: 1.1787\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.1130 - val_loss: 1.1736\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.1071 - val_loss: 1.1685\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.1024 - val_loss: 1.1635\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - ETA: 0s - loss: 1.102 - 1s 204us/step - loss: 1.0982 - val_loss: 1.1592\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0929 - val_loss: 1.1546\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0885 - val_loss: 1.1506\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0838 - val_loss: 1.1464\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0795 - val_loss: 1.1424\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0757 - val_loss: 1.1389\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0720 - val_loss: 1.1351\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.0683 - val_loss: 1.1314\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.0646 - val_loss: 1.1282\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0609 - val_loss: 1.1249\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0577 - val_loss: 1.1218\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0544 - val_loss: 1.1190\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0514 - val_loss: 1.1158\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0484 - val_loss: 1.1132\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0452 - val_loss: 1.1107\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0428 - val_loss: 1.1081\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0401 - val_loss: 1.1058\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0376 - val_loss: 1.1037\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0354 - val_loss: 1.1012\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0331 - val_loss: 1.0993\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0310 - val_loss: 1.0972\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0287 - val_loss: 1.0954\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0269 - val_loss: 1.0935\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0250 - val_loss: 1.0921\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0233 - val_loss: 1.0901\n",
      "threshold value :  -12.068000757129637\n",
      "Number of outliers :  18 \n",
      "Number of inliers :  1897\n",
      "Number of errors :  8\n",
      "Number of FP :  1\n",
      "Number of FN :  7\n",
      "\n",
      "----Contamination rate : 0.006044238683127572----\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_29 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 2s 619us/step - loss: 59.0536 - val_loss: 20.7222\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 14.6020 - val_loss: 10.8775\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 9.0165 - val_loss: 7.7268\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 6.7716 - val_loss: 6.1233\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 5.5618 - val_loss: 5.1546\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 4.7778 - val_loss: 4.5048\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 4.2343 - val_loss: 4.0338\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 3.8502 - val_loss: 3.6722\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 3.5417 - val_loss: 3.3903\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 3.2907 - val_loss: 3.1598\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 3.0801 - val_loss: 2.9701\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 233us/step - loss: 2.9160 - val_loss: 2.8095\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 2.7776 - val_loss: 2.6721\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 2.6390 - val_loss: 2.5523\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 2.5336 - val_loss: 2.4480\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 2.4366 - val_loss: 2.3547\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 214us/step - loss: 2.3469 - val_loss: 2.2718\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 2.2727 - val_loss: 2.1970\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 1s 256us/step - loss: 2.2002 - val_loss: 2.1297\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 244us/step - loss: 2.1361 - val_loss: 2.0681\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 239us/step - loss: 2.0778 - val_loss: 2.0103\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 2.0229 - val_loss: 1.9596\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.9751 - val_loss: 1.9109\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.9264 - val_loss: 1.8661\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.8849 - val_loss: 1.8253\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.8462 - val_loss: 1.7866\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.8078 - val_loss: 1.7505\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.7728 - val_loss: 1.7157\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.7395 - val_loss: 1.6841\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.7077 - val_loss: 1.6541\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.6776 - val_loss: 1.6252\n",
      "Epoch 32/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 237us/step - loss: 1.6521 - val_loss: 1.5981\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.6251 - val_loss: 1.5724\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 235us/step - loss: 1.5980 - val_loss: 1.5486\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 223us/step - loss: 1.5758 - val_loss: 1.5249\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.5537 - val_loss: 1.5037\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.5309 - val_loss: 1.4822\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.5101 - val_loss: 1.4620\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4899 - val_loss: 1.4430\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.4707 - val_loss: 1.4251\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4517 - val_loss: 1.4078\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.4371 - val_loss: 1.3908\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.4197 - val_loss: 1.3748\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.4010 - val_loss: 1.3588\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.3861 - val_loss: 1.3441\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.3695 - val_loss: 1.3302\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.3564 - val_loss: 1.3160\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.3424 - val_loss: 1.3031\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 208us/step - loss: 1.3292 - val_loss: 1.2902\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 208us/step - loss: 1.3165 - val_loss: 1.2784\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 1.3046 - val_loss: 1.2667\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.2930 - val_loss: 1.2551\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.2803 - val_loss: 1.2441\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 1.2700 - val_loss: 1.2337\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.2594 - val_loss: 1.2234\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.2490 - val_loss: 1.2136\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.2386 - val_loss: 1.2041\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.2294 - val_loss: 1.1952\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.2207 - val_loss: 1.1861\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 214us/step - loss: 1.2106 - val_loss: 1.1779\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.2027 - val_loss: 1.1696\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.1946 - val_loss: 1.1621\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.1861 - val_loss: 1.1540\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1786 - val_loss: 1.1467\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.1714 - val_loss: 1.1395\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 1.1638 - val_loss: 1.1328\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 232us/step - loss: 1.1568 - val_loss: 1.1262\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1500 - val_loss: 1.1202\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.1440 - val_loss: 1.1138\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.1373 - val_loss: 1.1079\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.1317 - val_loss: 1.1022\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.1260 - val_loss: 1.0967\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.1205 - val_loss: 1.0915\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1153 - val_loss: 1.0864\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.1098 - val_loss: 1.0815\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 1.1050 - val_loss: 1.0769\n",
      "Epoch 77/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.1000 - val_loss: 1.0723\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.0957 - val_loss: 1.0679\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.0913 - val_loss: 1.0639\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 265us/step - loss: 1.0872 - val_loss: 1.0598\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 242us/step - loss: 1.0832 - val_loss: 1.0561\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 228us/step - loss: 1.0792 - val_loss: 1.0522\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 233us/step - loss: 1.0752 - val_loss: 1.0486\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 228us/step - loss: 1.0720 - val_loss: 1.0453\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 277us/step - loss: 1.0685 - val_loss: 1.0418\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 264us/step - loss: 1.0647 - val_loss: 1.0387\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 230us/step - loss: 1.0616 - val_loss: 1.0357\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.0587 - val_loss: 1.0328\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 244us/step - loss: 1.0558 - val_loss: 1.0301\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 255us/step - loss: 1.0529 - val_loss: 1.0275\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.0504 - val_loss: 1.0252\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 263us/step - loss: 1.0477 - val_loss: 1.0226\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.0452 - val_loss: 1.0202\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.0430 - val_loss: 1.0180\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 242us/step - loss: 1.0407 - val_loss: 1.0159\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 372us/step - loss: 1.0385 - val_loss: 1.0139\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 365us/step - loss: 1.0366 - val_loss: 1.0120\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 335us/step - loss: 1.0346 - val_loss: 1.0102\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 334us/step - loss: 1.0329 - val_loss: 1.0085\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 243us/step - loss: 1.0311 - val_loss: 1.0070\n",
      "threshold value :  -13.451866643775666\n",
      "Number of outliers :  9 \n",
      "Number of inliers :  1906\n",
      "Number of errors :  15\n",
      "Number of FP :  0\n",
      "Number of FN :  15\n",
      "\n",
      "----Contamination rate : 0.02417695473251029----\n",
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_36 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 3s 774us/step - loss: 58.6549 - val_loss: 19.8465\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 272us/step - loss: 15.4000 - val_loss: 10.5527\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 9.5407 - val_loss: 7.6396\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 251us/step - loss: 7.1700 - val_loss: 6.0976\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 225us/step - loss: 5.8007 - val_loss: 5.1312\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 4.9567 - val_loss: 4.4783\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 4.3609 - val_loss: 4.0042\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 231us/step - loss: 3.9151 - val_loss: 3.6421\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 240us/step - loss: 3.5775 - val_loss: 3.3570\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 268us/step - loss: 3.3086 - val_loss: 3.1267\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 312us/step - loss: 3.0907 - val_loss: 2.9402\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 252us/step - loss: 2.9105 - val_loss: 2.7803\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 2.7596 - val_loss: 2.6455\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 349us/step - loss: 2.6265 - val_loss: 2.5283\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 275us/step - loss: 2.5159 - val_loss: 2.4279\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 354us/step - loss: 2.4221 - val_loss: 2.3381\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 322us/step - loss: 2.3282 - val_loss: 2.2572\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 3s 759us/step - loss: 2.2547 - val_loss: 2.1857\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 2s 709us/step - loss: 2.1853 - val_loss: 2.1197\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 334us/step - loss: 2.1142 - val_loss: 2.0594\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 303us/step - loss: 2.0592 - val_loss: 2.0064\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 372us/step - loss: 2.0131 - val_loss: 1.9564\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 380us/step - loss: 1.9604 - val_loss: 1.9102\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 381us/step - loss: 1.9127 - val_loss: 1.8679\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 392us/step - loss: 1.8634 - val_loss: 1.8281\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 2s 448us/step - loss: 1.8306 - val_loss: 1.7913\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 2s 594us/step - loss: 1.7927 - val_loss: 1.7573\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 396us/step - loss: 1.7611 - val_loss: 1.7248\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 377us/step - loss: 1.7259 - val_loss: 1.6941\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 301us/step - loss: 1.6964 - val_loss: 1.6650\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.6649 - val_loss: 1.6383\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.6403 - val_loss: 1.6118\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.6124 - val_loss: 1.5873\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 210us/step - loss: 1.5913 - val_loss: 1.5644\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.5642 - val_loss: 1.5421\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.5447 - val_loss: 1.5212\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.5229 - val_loss: 1.5011\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.5019 - val_loss: 1.4832\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.4833 - val_loss: 1.4646\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 230us/step - loss: 1.4639 - val_loss: 1.4465\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.4460 - val_loss: 1.4290\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 257us/step - loss: 1.4292 - val_loss: 1.4134\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 235us/step - loss: 1.4131 - val_loss: 1.3979\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 248us/step - loss: 1.3986 - val_loss: 1.3830\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.3821 - val_loss: 1.3692\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.3691 - val_loss: 1.3552\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.3539 - val_loss: 1.3416\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.3395 - val_loss: 1.3290\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 232us/step - loss: 1.3289 - val_loss: 1.3169\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.3142 - val_loss: 1.3049\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 214us/step - loss: 1.3024 - val_loss: 1.2943\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 1.2913 - val_loss: 1.2828\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 1.2788 - val_loss: 1.2722\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.2685 - val_loss: 1.2624\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 234us/step - loss: 1.2576 - val_loss: 1.2520\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 1.2488 - val_loss: 1.2432\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.2387 - val_loss: 1.2346\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.2293 - val_loss: 1.2247\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 213us/step - loss: 1.2208 - val_loss: 1.2161\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.2123 - val_loss: 1.2084\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 214us/step - loss: 1.2034 - val_loss: 1.1999\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 245us/step - loss: 1.1949 - val_loss: 1.1927\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 248us/step - loss: 1.1869 - val_loss: 1.1848\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 1.1791 - val_loss: 1.1776\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 215us/step - loss: 1.1715 - val_loss: 1.1711\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.1646 - val_loss: 1.1643\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 240us/step - loss: 1.1582 - val_loss: 1.1576\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 273us/step - loss: 1.1510 - val_loss: 1.1515\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.1450 - val_loss: 1.1455\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.1383 - val_loss: 1.1396\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 234us/step - loss: 1.1326 - val_loss: 1.1340\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.1272 - val_loss: 1.1287\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.1219 - val_loss: 1.1232\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1160 - val_loss: 1.1182\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.1108 - val_loss: 1.1134\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 226us/step - loss: 1.1061 - val_loss: 1.1088\n",
      "Epoch 77/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.1012 - val_loss: 1.1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.0965 - val_loss: 1.0999\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 214us/step - loss: 1.0918 - val_loss: 1.0956\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.0874 - val_loss: 1.0915\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0839 - val_loss: 1.0876\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.0796 - val_loss: 1.0839\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0758 - val_loss: 1.0805\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0721 - val_loss: 1.0769\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0688 - val_loss: 1.0734\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0655 - val_loss: 1.0702\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0619 - val_loss: 1.0672\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0591 - val_loss: 1.0642\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0563 - val_loss: 1.0613\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0531 - val_loss: 1.0587\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.0506 - val_loss: 1.0559\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.0477 - val_loss: 1.0534\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0450 - val_loss: 1.0510\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0426 - val_loss: 1.0488\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0405 - val_loss: 1.0466\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.0384 - val_loss: 1.0444\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0361 - val_loss: 1.0424\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0342 - val_loss: 1.0405\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.0323 - val_loss: 1.0387\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.0305 - val_loss: 1.0369\n",
      "threshold value :  -10.357520952186986\n",
      "Number of outliers :  50 \n",
      "Number of inliers :  1865\n",
      "Number of errors :  36\n",
      "Number of FP :  31\n",
      "Number of FN :  5\n",
      "\n",
      "----Contamination rate : 0.5----\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_43 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_41 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_42 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 2s 609us/step - loss: 55.8474 - val_loss: 20.9743\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 15.3782 - val_loss: 10.9652\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 9.3299 - val_loss: 7.7245\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 6.9032 - val_loss: 6.0773\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 5.6017 - val_loss: 5.0913\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 4.7895 - val_loss: 4.4313\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 4.2387 - val_loss: 3.9607\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 3.8251 - val_loss: 3.5985\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 3.5019 - val_loss: 3.3203\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 3.2522 - val_loss: 3.0950\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 3.0472 - val_loss: 2.9104\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 2.8754 - val_loss: 2.7552\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - ETA: 0s - loss: 2.749 - 1s 202us/step - loss: 2.7411 - val_loss: 2.6250\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 2.6158 - val_loss: 2.5107\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 2.5062 - val_loss: 2.4099\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 2.4072 - val_loss: 2.3213\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 2.3277 - val_loss: 2.2427\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 2.2491 - val_loss: 2.1726\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 1s 264us/step - loss: 2.1832 - val_loss: 2.1073\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 2.1148 - val_loss: 2.0488\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 2.0624 - val_loss: 1.9949\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 2.0073 - val_loss: 1.9456\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.9601 - val_loss: 1.8994\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 208us/step - loss: 1.9152 - val_loss: 1.8573\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.8797 - val_loss: 1.8176\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 217us/step - loss: 1.8343 - val_loss: 1.7804\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 1s 230us/step - loss: 1.7992 - val_loss: 1.7460\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.7594 - val_loss: 1.7135\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 262us/step - loss: 1.7320 - val_loss: 1.6831\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 248us/step - loss: 1.7008 - val_loss: 1.6548\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 262us/step - loss: 1.6710 - val_loss: 1.6271\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 1.6458 - val_loss: 1.6017\n",
      "Epoch 33/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 207us/step - loss: 1.6182 - val_loss: 1.5765\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.5930 - val_loss: 1.5533\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.5673 - val_loss: 1.5309\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.5477 - val_loss: 1.5100\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.5269 - val_loss: 1.4900\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 212us/step - loss: 1.5070 - val_loss: 1.4709\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.4871 - val_loss: 1.4524\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 253us/step - loss: 1.4651 - val_loss: 1.4346\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 279us/step - loss: 1.4482 - val_loss: 1.4184\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 314us/step - loss: 1.4328 - val_loss: 1.4018\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.4186 - val_loss: 1.3862\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.4009 - val_loss: 1.3714\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.3840 - val_loss: 1.3574\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.3706 - val_loss: 1.3434\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 223us/step - loss: 1.3595 - val_loss: 1.3299\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 232us/step - loss: 1.3438 - val_loss: 1.3173\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.3309 - val_loss: 1.3048\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.3176 - val_loss: 1.2926\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 224us/step - loss: 1.3038 - val_loss: 1.2814\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 238us/step - loss: 1.2930 - val_loss: 1.2702\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.2822 - val_loss: 1.2595\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 214us/step - loss: 1.2719 - val_loss: 1.2494\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.2613 - val_loss: 1.2399\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 211us/step - loss: 1.2512 - val_loss: 1.2302\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.2413 - val_loss: 1.2218\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 234us/step - loss: 1.2322 - val_loss: 1.2122\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 235us/step - loss: 1.2230 - val_loss: 1.2040\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.2139 - val_loss: 1.1951\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - ETA: 0s - loss: 1.209 - 1s 249us/step - loss: 1.2063 - val_loss: 1.1871\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 225us/step - loss: 1.1966 - val_loss: 1.1797\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 219us/step - loss: 1.1889 - val_loss: 1.1719\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.1815 - val_loss: 1.1646\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - ETA: 0s - loss: 1.174 - 1s 238us/step - loss: 1.1747 - val_loss: 1.1577\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 270us/step - loss: 1.1674 - val_loss: 1.1509\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 272us/step - loss: 1.1600 - val_loss: 1.1446\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.1531 - val_loss: 1.1382\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 2s 502us/step - loss: 1.1469 - val_loss: 1.1323\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 371us/step - loss: 1.1406 - val_loss: 1.1266\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 2s 587us/step - loss: 1.1355 - val_loss: 1.1209\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 368us/step - loss: 1.1291 - val_loss: 1.1154\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 3s 715us/step - loss: 1.1239 - val_loss: 1.1101\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 305us/step - loss: 1.1181 - val_loss: 1.1051\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 299us/step - loss: 1.1132 - val_loss: 1.1005\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.1080 - val_loss: 1.0956\n",
      "Epoch 77/100\n",
      "3499/3499 [==============================] - 1s 268us/step - loss: 1.1032 - val_loss: 1.0913\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 268us/step - loss: 1.0983 - val_loss: 1.0867\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.0940 - val_loss: 1.0826\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.0901 - val_loss: 1.0784\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 226us/step - loss: 1.0858 - val_loss: 1.0745\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 222us/step - loss: 1.0816 - val_loss: 1.0709\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 1.0775 - val_loss: 1.0672\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 220us/step - loss: 1.0742 - val_loss: 1.0639\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 1.0709 - val_loss: 1.0605\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.0671 - val_loss: 1.0572\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 216us/step - loss: 1.0640 - val_loss: 1.0543\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 218us/step - loss: 1.0607 - val_loss: 1.0512\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 237us/step - loss: 1.0579 - val_loss: 1.0485\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 232us/step - loss: 1.0549 - val_loss: 1.0457\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 223us/step - loss: 1.0521 - val_loss: 1.0433\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 230us/step - loss: 1.0494 - val_loss: 1.0407\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.0471 - val_loss: 1.0382\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.0444 - val_loss: 1.0360\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.0421 - val_loss: 1.0338\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 221us/step - loss: 1.0400 - val_loss: 1.0317\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 227us/step - loss: 1.0379 - val_loss: 1.0297\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 229us/step - loss: 1.0358 - val_loss: 1.0278\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.0339 - val_loss: 1.0259\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 234us/step - loss: 1.0320 - val_loss: 1.0242\n",
      "threshold value :  -5.504318790288279\n",
      "Number of outliers :  962 \n",
      "Number of inliers :  953\n",
      "Number of errors :  938\n",
      "Number of FP :  938\n",
      "Number of FN :  0\n",
      "\n",
      "----Contamination rate : 0.99----\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 36)                1332      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 36)                0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 64)                2368      \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 36)                2340      \n",
      "=================================================================\n",
      "Total params: 12,620\n",
      "Trainable params: 12,620\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 389 samples\n",
      "Epoch 1/100\n",
      "3499/3499 [==============================] - 3s 737us/step - loss: 53.2549 - val_loss: 18.8760\n",
      "Epoch 2/100\n",
      "3499/3499 [==============================] - 1s 189us/step - loss: 14.5548 - val_loss: 10.3835\n",
      "Epoch 3/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 9.0502 - val_loss: 7.4004\n",
      "Epoch 4/100\n",
      "3499/3499 [==============================] - 1s 191us/step - loss: 6.7452 - val_loss: 5.8424\n",
      "Epoch 5/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 5.4760 - val_loss: 4.8987\n",
      "Epoch 6/100\n",
      "3499/3499 [==============================] - 1s 190us/step - loss: 4.7087 - val_loss: 4.2753\n",
      "Epoch 7/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 4.1878 - val_loss: 3.8266\n",
      "Epoch 8/100\n",
      "3499/3499 [==============================] - 1s 259us/step - loss: 3.7818 - val_loss: 3.4887\n",
      "Epoch 9/100\n",
      "3499/3499 [==============================] - 1s 230us/step - loss: 3.4787 - val_loss: 3.2233\n",
      "Epoch 10/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 3.2321 - val_loss: 3.0063\n",
      "Epoch 11/100\n",
      "3499/3499 [==============================] - ETA: 0s - loss: 3.050 - 1s 224us/step - loss: 3.0390 - val_loss: 2.8270\n",
      "Epoch 12/100\n",
      "3499/3499 [==============================] - 1s 252us/step - loss: 2.8668 - val_loss: 2.6746\n",
      "Epoch 13/100\n",
      "3499/3499 [==============================] - 1s 352us/step - loss: 2.7278 - val_loss: 2.5453\n",
      "Epoch 14/100\n",
      "3499/3499 [==============================] - 1s 345us/step - loss: 2.6010 - val_loss: 2.4328\n",
      "Epoch 15/100\n",
      "3499/3499 [==============================] - 1s 305us/step - loss: 2.4958 - val_loss: 2.3337\n",
      "Epoch 16/100\n",
      "3499/3499 [==============================] - 1s 262us/step - loss: 2.3998 - val_loss: 2.2462\n",
      "Epoch 17/100\n",
      "3499/3499 [==============================] - 1s 292us/step - loss: 2.3172 - val_loss: 2.1684\n",
      "Epoch 18/100\n",
      "3499/3499 [==============================] - 1s 282us/step - loss: 2.2457 - val_loss: 2.0977\n",
      "Epoch 19/100\n",
      "3499/3499 [==============================] - 1s 253us/step - loss: 2.1736 - val_loss: 2.0330\n",
      "Epoch 20/100\n",
      "3499/3499 [==============================] - 1s 303us/step - loss: 2.1156 - val_loss: 1.9754\n",
      "Epoch 21/100\n",
      "3499/3499 [==============================] - 1s 270us/step - loss: 2.0537 - val_loss: 1.9229\n",
      "Epoch 22/100\n",
      "3499/3499 [==============================] - 1s 234us/step - loss: 1.9983 - val_loss: 1.8726\n",
      "Epoch 23/100\n",
      "3499/3499 [==============================] - 1s 236us/step - loss: 1.9536 - val_loss: 1.8275\n",
      "Epoch 24/100\n",
      "3499/3499 [==============================] - 1s 246us/step - loss: 1.9112 - val_loss: 1.7859\n",
      "Epoch 25/100\n",
      "3499/3499 [==============================] - 1s 239us/step - loss: 1.8633 - val_loss: 1.7464\n",
      "Epoch 26/100\n",
      "3499/3499 [==============================] - 1s 247us/step - loss: 1.8257 - val_loss: 1.7099\n",
      "Epoch 27/100\n",
      "3499/3499 [==============================] - 1s 266us/step - loss: 1.7910 - val_loss: 1.6765\n",
      "Epoch 28/100\n",
      "3499/3499 [==============================] - 1s 209us/step - loss: 1.7573 - val_loss: 1.6434\n",
      "Epoch 29/100\n",
      "3499/3499 [==============================] - 1s 204us/step - loss: 1.7276 - val_loss: 1.6130\n",
      "Epoch 30/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.6934 - val_loss: 1.5845\n",
      "Epoch 31/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.6674 - val_loss: 1.5583\n",
      "Epoch 32/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.6359 - val_loss: 1.5320\n",
      "Epoch 33/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.6137 - val_loss: 1.5077\n",
      "Epoch 34/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.5898 - val_loss: 1.4845\n",
      "Epoch 35/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.5649 - val_loss: 1.4640\n",
      "Epoch 36/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.5440 - val_loss: 1.4420\n",
      "Epoch 37/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.5252 - val_loss: 1.4231\n",
      "Epoch 38/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.5022 - val_loss: 1.4036\n",
      "Epoch 39/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4832 - val_loss: 1.3854\n",
      "Epoch 40/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.4653 - val_loss: 1.3677\n",
      "Epoch 41/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.4449 - val_loss: 1.3515\n",
      "Epoch 42/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.4301 - val_loss: 1.3350\n",
      "Epoch 43/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.4130 - val_loss: 1.3198\n",
      "Epoch 44/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.4004 - val_loss: 1.3062\n",
      "Epoch 45/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.3823 - val_loss: 1.2911\n",
      "Epoch 46/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.3690 - val_loss: 1.2781\n",
      "Epoch 47/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.3540 - val_loss: 1.2651\n",
      "Epoch 48/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.3404 - val_loss: 1.2517\n",
      "Epoch 49/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.3282 - val_loss: 1.2403\n",
      "Epoch 50/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.3173 - val_loss: 1.2278\n",
      "Epoch 51/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.3049 - val_loss: 1.2167\n",
      "Epoch 52/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.2931 - val_loss: 1.2062\n",
      "Epoch 53/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2834 - val_loss: 1.1957\n",
      "Epoch 54/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2710 - val_loss: 1.1855\n",
      "Epoch 55/100\n",
      "3499/3499 [==============================] - 1s 206us/step - loss: 1.2606 - val_loss: 1.1756\n",
      "Epoch 56/100\n",
      "3499/3499 [==============================] - 1s 205us/step - loss: 1.2506 - val_loss: 1.1659\n",
      "Epoch 57/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.2408 - val_loss: 1.1569\n",
      "Epoch 58/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.2304 - val_loss: 1.1489\n",
      "Epoch 59/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.2231 - val_loss: 1.1398\n",
      "Epoch 60/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.2136 - val_loss: 1.1317\n",
      "Epoch 61/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.2051 - val_loss: 1.1237\n",
      "Epoch 62/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.1979 - val_loss: 1.1162\n",
      "Epoch 63/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.1888 - val_loss: 1.1088\n",
      "Epoch 64/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.1821 - val_loss: 1.1018\n",
      "Epoch 65/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1752 - val_loss: 1.0948\n",
      "Epoch 66/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.1681 - val_loss: 1.0883\n",
      "Epoch 67/100\n",
      "3499/3499 [==============================] - 1s 202us/step - loss: 1.1605 - val_loss: 1.0818\n",
      "Epoch 68/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.1555 - val_loss: 1.0755\n",
      "Epoch 69/100\n",
      "3499/3499 [==============================] - 1s 201us/step - loss: 1.1490 - val_loss: 1.0698\n",
      "Epoch 70/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.1421 - val_loss: 1.0642\n",
      "Epoch 71/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 1.1360 - val_loss: 1.0584\n",
      "Epoch 72/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.1300 - val_loss: 1.0531\n",
      "Epoch 73/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.1253 - val_loss: 1.0480\n",
      "Epoch 74/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1200 - val_loss: 1.0435\n",
      "Epoch 75/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.1148 - val_loss: 1.0384\n",
      "Epoch 76/100\n",
      "3499/3499 [==============================] - 1s 199us/step - loss: 1.1098 - val_loss: 1.0336\n",
      "Epoch 77/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 1s 193us/step - loss: 1.1043 - val_loss: 1.0292\n",
      "Epoch 78/100\n",
      "3499/3499 [==============================] - 1s 191us/step - loss: 1.1004 - val_loss: 1.0251\n",
      "Epoch 79/100\n",
      "3499/3499 [==============================] - 1s 198us/step - loss: 1.0963 - val_loss: 1.0209\n",
      "Epoch 80/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0922 - val_loss: 1.0169\n",
      "Epoch 81/100\n",
      "3499/3499 [==============================] - 1s 192us/step - loss: 1.0877 - val_loss: 1.0131\n",
      "Epoch 82/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0835 - val_loss: 1.0095\n",
      "Epoch 83/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0802 - val_loss: 1.0061\n",
      "Epoch 84/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0767 - val_loss: 1.0028\n",
      "Epoch 85/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0734 - val_loss: 0.9995\n",
      "Epoch 86/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0698 - val_loss: 0.9964\n",
      "Epoch 87/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0668 - val_loss: 0.9933\n",
      "Epoch 88/100\n",
      "3499/3499 [==============================] - 1s 192us/step - loss: 1.0636 - val_loss: 0.9907\n",
      "Epoch 89/100\n",
      "3499/3499 [==============================] - 1s 192us/step - loss: 1.0610 - val_loss: 0.9879\n",
      "Epoch 90/100\n",
      "3499/3499 [==============================] - 1s 203us/step - loss: 1.0580 - val_loss: 0.9852\n",
      "Epoch 91/100\n",
      "3499/3499 [==============================] - 1s 197us/step - loss: 1.0555 - val_loss: 0.9829\n",
      "Epoch 92/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0528 - val_loss: 0.9802\n",
      "Epoch 93/100\n",
      "3499/3499 [==============================] - 1s 200us/step - loss: 1.0505 - val_loss: 0.9779\n",
      "Epoch 94/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 1.0481 - val_loss: 0.9759\n",
      "Epoch 95/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0458 - val_loss: 0.9737\n",
      "Epoch 96/100\n",
      "3499/3499 [==============================] - 1s 196us/step - loss: 1.0437 - val_loss: 0.9717\n",
      "Epoch 97/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.0416 - val_loss: 0.9698\n",
      "Epoch 98/100\n",
      "3499/3499 [==============================] - 1s 193us/step - loss: 1.0399 - val_loss: 0.9680\n",
      "Epoch 99/100\n",
      "3499/3499 [==============================] - 1s 195us/step - loss: 1.0380 - val_loss: 0.9664\n",
      "Epoch 100/100\n",
      "3499/3499 [==============================] - 1s 194us/step - loss: 1.0362 - val_loss: 0.9645\n",
      "threshold value :  -2.2942520705534566\n",
      "Number of outliers :  1898 \n",
      "Number of inliers :  17\n",
      "Number of errors :  1874\n",
      "Number of FP :  1874\n",
      "Number of FN :  0\n"
     ]
    }
   ],
   "source": [
    "dict_clf={}\n",
    "df_result = pd.DataFrame({\"y_test\" : np.array(y_test).astype(int)})\n",
    "dict_result={}\n",
    "\n",
    "for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "    print(\"\\n---------------------\"+clf_name+\"---------------------\")\n",
    "    print(\"Classifier with correct contamination : \", clf)\n",
    "    dict_result[clf_name]={}\n",
    "    dict_clf[clf_name]={}\n",
    "    \n",
    "    for contamination in args[\"contamination\"]:\n",
    "        print('\\n----Contamination rate : '+str(contamination)+'----')\n",
    "        \n",
    "        # update contamination of the classifier\n",
    "        clf.contamination=contamination\n",
    "        \n",
    "        # define contamination name (for backup purpose)\n",
    "        if contamination == outlier_fraction:\n",
    "            contamination_name = \"outlier\"\n",
    "        else :\n",
    "            contamination_name = str(contamination)\n",
    "        dict_result[clf_name][contamination_name]={}\n",
    "        \n",
    "        # Fit on training data\n",
    "        clf.fit(X_train)\n",
    "        dict_clf[clf_name][contamination_name]=clf\n",
    "        \n",
    "        # Predict raw anomaly score on test data\n",
    "        scores_pred = -clf.decision_function(X_test) # take the negative to get a positive number for an anomaly\n",
    "        df_result[str(\"score_\"+clf_name+contamination_name)] = scores_pred\n",
    "\n",
    "        # Threshold value to consider a datapoint inlier or outlier\n",
    "        threshold = stats.scoreatpercentile(scores_pred,100 * contamination)\n",
    "        print(\"threshold value : \", threshold)\n",
    "#         df_result[str(\"thres_\"+clf_name+contamination_name)] = threshold\n",
    "        dict_result[clf_name][contamination_name][\"thres\"] = threshold\n",
    "        \n",
    "        # Prediction \n",
    "        y_pred = clf.predict(X_test)\n",
    "        n_inliers = len(y_pred) - np.count_nonzero(y_pred)\n",
    "        n_outliers = np.count_nonzero(y_pred == 1)\n",
    "        df_result[str(\"y_pred_\"+clf_name+contamination_name)] = y_pred.tolist()\n",
    "        print('Number of outliers : ',n_outliers,'\\nNumber of inliers : ',n_inliers)\n",
    "\n",
    "        # no of errors in prediction\n",
    "        n_errors = np.sum(y_pred != np.array(y_test))\n",
    "        print('Number of errors : ',n_errors)\n",
    "        dict_result[clf_name][contamination_name][\"n_errors\"] = n_errors\n",
    "        FP = np.sum((np.array(y_test)==0) & (y_pred==1))\n",
    "        print(\"Number of FP : \",FP)\n",
    "        dict_result[clf_name][contamination_name][\"FP\"] = FP\n",
    "        FN = np.sum((np.array(y_test)==1) & (y_pred==0))\n",
    "        print(\"Number of FN : \",FN)\n",
    "        dict_result[clf_name][contamination_name][\"FN\"] = FN\n",
    "        \n",
    "#         # plot confusion matrix\n",
    "#         plot_confusion(y_pred, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(dict_result, metric, remove_contam_beg = 0, remove_contam_end = 0) :\n",
    "    for clf_name in dict_result.keys() :\n",
    "        metric_list = []\n",
    "        if remove_contam_end == 0 :\n",
    "            contam_list = list(dict_result[clf_name].keys())[remove_contam_beg:]\n",
    "            contam_nb = args[\"contamination\"][remove_contam_beg:]\n",
    "        else :\n",
    "            contam_list = list(dict_result[clf_name].keys())[remove_contam_beg:-remove_contam_end]\n",
    "            contam_nb = args[\"contamination\"][remove_contam_beg:-remove_contam_end]\n",
    "        for contamination in contam_list:\n",
    "            try :\n",
    "                metric_list.append(dict_result[clf_name][contamination][metric])\n",
    "            except :\n",
    "                break\n",
    "        plt.scatter(np.log(contam_nb),metric_list,label = clf_name)\n",
    "    plt.title(metric)\n",
    "    plt.axvline(np.log(outlier_fraction), label='true outlier fraction')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(dict_result, \"FP\")\n",
    "plot_result(dict_result, \"FP\", remove_contam_beg = 2, remove_contam_end =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(dict_result, \"FN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(dict_result, \"n_errors\",remove_contam_end = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>score_ABODoutlier</th>\n",
       "      <th>y_pred_ABODoutlier</th>\n",
       "      <th>score_ABOD1e-10</th>\n",
       "      <th>y_pred_ABOD1e-10</th>\n",
       "      <th>score_ABOD0.001</th>\n",
       "      <th>y_pred_ABOD0.001</th>\n",
       "      <th>score_ABOD0.01</th>\n",
       "      <th>y_pred_ABOD0.01</th>\n",
       "      <th>score_ABOD0.006044238683127572</th>\n",
       "      <th>...</th>\n",
       "      <th>score_AutoEncoder0.01</th>\n",
       "      <th>y_pred_AutoEncoder0.01</th>\n",
       "      <th>score_AutoEncoder0.006044238683127572</th>\n",
       "      <th>y_pred_AutoEncoder0.006044238683127572</th>\n",
       "      <th>score_AutoEncoder0.02417695473251029</th>\n",
       "      <th>y_pred_AutoEncoder0.02417695473251029</th>\n",
       "      <th>score_AutoEncoder0.5</th>\n",
       "      <th>y_pred_AutoEncoder0.5</th>\n",
       "      <th>score_AutoEncoder0.99</th>\n",
       "      <th>y_pred_AutoEncoder0.99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3.535245e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.535245e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.535245e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.535245e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.535245e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.395108</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.394844</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.395612</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.395066</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.395167</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3.251472e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.251472e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.251472e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.251472e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>3.251472e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.197067</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.194390</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.197777</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.196357</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.197122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3.141807e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.141807e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.141807e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.141807e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>3.141807e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.441740</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.443570</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.441088</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.442178</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.441589</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>9.789379e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>9.789379e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>9.789379e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>9.789379e-08</td>\n",
       "      <td>0</td>\n",
       "      <td>9.789379e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.637701</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.635261</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.637759</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.636869</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.637535</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.283796e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.283796e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.283796e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.283796e-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2.283796e-07</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.081018</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.078163</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.081449</td>\n",
       "      <td>0</td>\n",
       "      <td>-8.080104</td>\n",
       "      <td>1</td>\n",
       "      <td>-8.081055</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   y_test  score_ABODoutlier  y_pred_ABODoutlier  score_ABOD1e-10  \\\n",
       "0       0       3.535245e-08                   0     3.535245e-08   \n",
       "1       0       3.251472e-07                   0     3.251472e-07   \n",
       "2       0       3.141807e-08                   0     3.141807e-08   \n",
       "3       0       9.789379e-08                   0     9.789379e-08   \n",
       "4       0       2.283796e-07                   0     2.283796e-07   \n",
       "\n",
       "   y_pred_ABOD1e-10  score_ABOD0.001  y_pred_ABOD0.001  score_ABOD0.01  \\\n",
       "0                 0     3.535245e-08                 0    3.535245e-08   \n",
       "1                 0     3.251472e-07                 0    3.251472e-07   \n",
       "2                 0     3.141807e-08                 0    3.141807e-08   \n",
       "3                 0     9.789379e-08                 0    9.789379e-08   \n",
       "4                 0     2.283796e-07                 0    2.283796e-07   \n",
       "\n",
       "   y_pred_ABOD0.01  score_ABOD0.006044238683127572  ...  \\\n",
       "0                0                    3.535245e-08  ...   \n",
       "1                0                    3.251472e-07  ...   \n",
       "2                0                    3.141807e-08  ...   \n",
       "3                0                    9.789379e-08  ...   \n",
       "4                0                    2.283796e-07  ...   \n",
       "\n",
       "   score_AutoEncoder0.01  y_pred_AutoEncoder0.01  \\\n",
       "0              -3.395108                       0   \n",
       "1              -7.197067                       0   \n",
       "2              -6.441740                       0   \n",
       "3              -3.637701                       0   \n",
       "4              -8.081018                       0   \n",
       "\n",
       "   score_AutoEncoder0.006044238683127572  \\\n",
       "0                              -3.394844   \n",
       "1                              -7.194390   \n",
       "2                              -6.443570   \n",
       "3                              -3.635261   \n",
       "4                              -8.078163   \n",
       "\n",
       "   y_pred_AutoEncoder0.006044238683127572  \\\n",
       "0                                       0   \n",
       "1                                       0   \n",
       "2                                       0   \n",
       "3                                       0   \n",
       "4                                       0   \n",
       "\n",
       "   score_AutoEncoder0.02417695473251029  \\\n",
       "0                             -3.395612   \n",
       "1                             -7.197777   \n",
       "2                             -6.441088   \n",
       "3                             -3.637759   \n",
       "4                             -8.081449   \n",
       "\n",
       "   y_pred_AutoEncoder0.02417695473251029  score_AutoEncoder0.5  \\\n",
       "0                                      0             -3.395066   \n",
       "1                                      0             -7.196357   \n",
       "2                                      0             -6.442178   \n",
       "3                                      0             -3.636869   \n",
       "4                                      0             -8.080104   \n",
       "\n",
       "   y_pred_AutoEncoder0.5  score_AutoEncoder0.99  y_pred_AutoEncoder0.99  \n",
       "0                      0              -3.395167                       1  \n",
       "1                      1              -7.197122                       1  \n",
       "2                      1              -6.441589                       1  \n",
       "3                      0              -3.637535                       1  \n",
       "4                      1              -8.081055                       1  \n",
       "\n",
       "[5 rows x 97 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl.dump(df_result,open('pickles/df_result.pkl', 'wb') )\n",
    "pkl.dump(dict_result,open('pickles/dict_result.pkl', 'wb') )\n",
    "df_result.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
